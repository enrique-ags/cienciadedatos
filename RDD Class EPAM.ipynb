{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ccef57",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561fbb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products between 1 and 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 99999/99999 [00:00<00:00, 654628.24it/s]\n",
      "100%|█████████████████████████████████████████| 9/9 [00:00<00:00, 166293.99it/s]\n",
      "100%|███████████████████████████████████████████| 40/40 [12:01<00:00, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "letters = string.ascii_lowercase\n",
    "letters_upper = string.ascii_uppercase\n",
    "for _i in range(0, 10):\n",
    "    letters += letters\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters_upper\n",
    "\n",
    "\n",
    "def random_string(stringLength=10):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "\n",
    "print(\"Products between {} and {}\".format(1, 100000))\n",
    "product_ids = [x for x in range(1, 100000)]\n",
    "dates = ['2020-07-01', '2020-07-02', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08', '2020-07-09', '2020-07-10']\n",
    "seller_ids = [x for x in range(1, 10)]\n",
    "\n",
    "\n",
    "#   Generate products\n",
    "products = [[0, \"product_0\", 22]]\n",
    "for p in tqdm(product_ids):\n",
    "    products.append([p, \"product_{}\".format(p), random.randint(1, 150)])\n",
    "#   Save dataframe\n",
    "df = pd.DataFrame(products)\n",
    "df.columns = [\"product_id\", \"product_name\", \"price\"]\n",
    "df.to_csv(\"products.csv\", index=False)\n",
    "del df\n",
    "del products\n",
    "\n",
    "#   Generate sellers\n",
    "sellers = [[0, \"seller_0\", 100000]]\n",
    "for s in tqdm(seller_ids):\n",
    "    sellers.append([s, \"seller_{}\".format(s), random.randint(0, 100000)])\n",
    "#   Save dataframe\n",
    "df = pd.DataFrame(sellers)\n",
    "df.columns = [\"seller_id\", \"seller_name\", \"daily_target\"]\n",
    "df.to_csv(\"sellers.csv\", index=False)\n",
    "\n",
    "#   Generate sales\n",
    "total_rows = 100000\n",
    "prod_zero = int(total_rows * 0.95)\n",
    "prod_others = total_rows - prod_zero + 1\n",
    "df_array = [[\"order_id\", \"product_id\", \"seller_id\", \"date\", \"num_pieces_sold\", \"bill_raw_text\"]]\n",
    "with open('sales.csv', 'w', newline='') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerows(df_array)\n",
    "\n",
    "order_id = 0\n",
    "for i in tqdm(range(0, 40)):\n",
    "    df_array = []\n",
    "\n",
    "    for i in range(0, prod_zero):\n",
    "        order_id += 1\n",
    "        df_array.append([order_id, 0, 0, random.choice(dates), random.randint(1, 100), random_string(500)])\n",
    "\n",
    "    with open('sales.csv', 'a', newline='') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerows(df_array)\n",
    "\n",
    "    df_array = []\n",
    "    for i in range(0, prod_others):\n",
    "        order_id += 1\n",
    "        df_array.append(\n",
    "            [order_id, random.choice(product_ids), random.choice(seller_ids), random.choice(dates),\n",
    "             random.randint(1, 100), random_string(500)])\n",
    "\n",
    "    with open('sales.csv', 'a', newline='') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerows(df_array)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605d6554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/15 12:49:17 WARN Utils: Your hostname, Enriques-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.74 instead (on interface en0)\n",
      "23/09/15 12:49:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/15 12:49:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Epam_academy').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de6e4b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Epam_academy</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe150017be0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0eaf35",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bd02bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1. Please upload data from sellers.csv into sellers_rdd variable. Use the textFile method with map function: \n",
    "#sc.textFile(<your_path>).map(lambda x: x.split(\",\"))\n",
    "\n",
    "sellers_rdd = spark.sparkContext.textFile('sellers.csv').map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f05b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['seller_id', 'seller_name', 'daily_target'],\n",
       " ['0', 'seller_0', '100000'],\n",
       " ['1', 'seller_1', '83478'],\n",
       " ['2', 'seller_2', '94114'],\n",
       " ['3', 'seller_3', '50299'],\n",
       " ['4', 'seller_4', '72654'],\n",
       " ['5', 'seller_5', '28862'],\n",
       " ['6', 'seller_6', '61878'],\n",
       " ['7', 'seller_7', '72047'],\n",
       " ['8', 'seller_8', '54715']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.2. Take first 10 elements of sellers_rdd.\n",
    "sellers_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b8acfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3. Save the first element of sellers_rdd into sellers_rdd_header variable\n",
    "sellers_rdd_header  = sellers_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86527131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['seller_id', 'seller_name', 'daily_target']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the header\n",
    "sellers_rdd_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37bafe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4. Filter the sellers_rdd excluding the sellers_rdd_header value, and reassign \n",
    "#it to sellers_rdd. Hint: use the != operator. Now sellers_rdd shouldn’t contain a header.\n",
    "sellers_rdd=(sellers_rdd.filter(lambda row: row!= sellers_rdd_header[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0f157e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', 'seller_0', '100000'],\n",
       " ['1', 'seller_1', '83478'],\n",
       " ['2', 'seller_2', '94114'],\n",
       " ['3', 'seller_3', '50299'],\n",
       " ['4', 'seller_4', '72654'],\n",
       " ['5', 'seller_5', '28862'],\n",
       " ['6', 'seller_6', '61878'],\n",
       " ['7', 'seller_7', '72047'],\n",
       " ['8', 'seller_8', '54715'],\n",
       " ['9', 'seller_9', '82824']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no header\n",
    "sellers_rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5047b6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 'seller_0', 100000),\n",
       " ('1', 'seller_1', 83478),\n",
       " ('2', 'seller_2', 94114),\n",
       " ('3', 'seller_3', 50299),\n",
       " ('4', 'seller_4', 72654)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.5.1. Take top 3 elements of sellers_rdd in descending order.\n",
    "sellers_rdd_sorted = sellers_rdd.map(lambda x: (x[0],x[1],int(x[2])))\n",
    "sellers_rdd_sorted.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59e1d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5.1. Take top 3 elements of sellers_rdd in descending order.\n",
    "rdd2 = sellers_rdd_sorted.sortBy(lambda x: x[2],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fc6bc7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 'seller_0', 100000), ('2', 'seller_2', 94114), ('1', 'seller_1', 83478)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3) # order in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7970c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5.2. Take top 3 elements of sellers_rdd in ascending order.\n",
    "rdd2 = sellers_rdd_sorted.sortBy(lambda x: x[2],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f54a406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', 'seller_5', 28862), ('3', 'seller_3', 50299), ('8', 'seller_8', 54715)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3) # order in asc order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60606d9e",
   "metadata": {},
   "source": [
    "# TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3e1d8638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.1. Please create the calendar_rdd variable containing years starting\n",
    "#from 1970 to 2099 using parallelized collections\n",
    "#\n",
    "x = list(range(1970, 2099+1))\n",
    "calendar_rdd = spark.sparkContext.parallelize(x)\n",
    "calendar_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6fb667fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    }
   ],
   "source": [
    "#3.2. Filter the calendar_rdd data containing only coming years (2022, 2023..) and save it to the calendar_coming_years_rdd variable. \n",
    "#Please don’t hardcode the current year, use Python code to generate it automatically.\n",
    "import datetime\n",
    "current_year = datetime.datetime.now()\n",
    "current_year = current_year.year\n",
    "print(current_year)\n",
    "\n",
    "calendar_coming_years_rdd = calendar_rdd.filter(lambda y: y>=current_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "28b40619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2099"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_coming_years_rdd.max() #should return the max item , should be 2099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "167a3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.3. Filter the calendar_rdd data containing only leap years (1972…2000…2016, 2020, 2024, …)\n",
    "#and save it to the calendar_leap_years_rdd variable.\n",
    "\n",
    "import calendar\n",
    "\n",
    "# Function to check if a year is a leap year\n",
    "def is_leap_year(year):\n",
    "    return calendar.isleap(year)\n",
    "\n",
    "list_year = calendar_rdd.collect()\n",
    "list_leap_year = []\n",
    "\n",
    "for x in list_year:\n",
    "    if is_leap_year(x):\n",
    "        list_leap_year.append(x)\n",
    "\n",
    "\n",
    "calendar_leap_years_rdd = spark.sparkContext.parallelize(list_leap_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a4dccccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max year:  2099\n",
      "list of Leap years:  [1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, 2024, 2028, 2032, 2036, 2040, 2044, 2048, 2052, 2056, 2060, 2064, 2068, 2072, 2076, 2080, 2084, 2088, 2092, 2096]\n",
      "calendar_leap_years_rdd its a kind of   <class 'pyspark.rdd.RDD'>  :  ParallelCollectionRDD[153] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    }
   ],
   "source": [
    "print('Max year: ',list_year[-1])\n",
    "print('list of Leap years: ',list_leap_year)\n",
    "print('calendar_leap_years_rdd its a kind of  ', type(calendar_leap_years_rdd),' : ', calendar_leap_years_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "79f83636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1972,\n",
       " 1976,\n",
       " 1980,\n",
       " 1984,\n",
       " 1988,\n",
       " 1992,\n",
       " 1996,\n",
       " 2000,\n",
       " 2004,\n",
       " 2008,\n",
       " 2012,\n",
       " 2016,\n",
       " 2020,\n",
       " 2024,\n",
       " 2028,\n",
       " 2032,\n",
       " 2036,\n",
       " 2040,\n",
       " 2044,\n",
       " 2048,\n",
       " 2052,\n",
       " 2056,\n",
       " 2060,\n",
       " 2064,\n",
       " 2068,\n",
       " 2072,\n",
       " 2076,\n",
       " 2080,\n",
       " 2084,\n",
       " 2088,\n",
       " 2092,\n",
       " 2096]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_leap_years_rdd.take(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a2c2ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1970,\n",
       " 1971,\n",
       " 1973,\n",
       " 1974,\n",
       " 1975,\n",
       " 1977,\n",
       " 1978,\n",
       " 1979,\n",
       " 1981,\n",
       " 1982,\n",
       " 1983,\n",
       " 1985,\n",
       " 1986,\n",
       " 1987,\n",
       " 1989,\n",
       " 1990,\n",
       " 1991,\n",
       " 1993,\n",
       " 1994,\n",
       " 1995,\n",
       " 1997,\n",
       " 1998,\n",
       " 1999,\n",
       " 2001,\n",
       " 2002,\n",
       " 2003,\n",
       " 2005,\n",
       " 2006,\n",
       " 2007,\n",
       " 2009,\n",
       " 2010,\n",
       " 2011,\n",
       " 2013,\n",
       " 2014,\n",
       " 2015,\n",
       " 2017,\n",
       " 2018,\n",
       " 2019,\n",
       " 2021,\n",
       " 2022,\n",
       " 2023,\n",
       " 2025,\n",
       " 2026,\n",
       " 2027,\n",
       " 2029,\n",
       " 2030,\n",
       " 2031,\n",
       " 2033,\n",
       " 2034,\n",
       " 2035,\n",
       " 2037,\n",
       " 2038,\n",
       " 2039,\n",
       " 2041,\n",
       " 2042,\n",
       " 2043,\n",
       " 2045,\n",
       " 2046,\n",
       " 2047,\n",
       " 2049,\n",
       " 2050,\n",
       " 2051,\n",
       " 2053,\n",
       " 2054,\n",
       " 2055,\n",
       " 2057,\n",
       " 2058,\n",
       " 2059,\n",
       " 2061,\n",
       " 2062,\n",
       " 2063,\n",
       " 2065,\n",
       " 2066,\n",
       " 2067,\n",
       " 2069,\n",
       " 2070,\n",
       " 2071,\n",
       " 2073,\n",
       " 2074,\n",
       " 2075,\n",
       " 2077,\n",
       " 2078,\n",
       " 2079,\n",
       " 2081,\n",
       " 2082,\n",
       " 2083,\n",
       " 2085,\n",
       " 2086,\n",
       " 2087,\n",
       " 2089,\n",
       " 2090,\n",
       " 2091,\n",
       " 2093,\n",
       " 2094,\n",
       " 2095,\n",
       " 2097,\n",
       " 2098,\n",
       " 2099]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.4. Create calendar_not_leap_years_rdd using some of SET operator and calendar_rdd and calendar_leap_years_rdd. It should contain all not leap years. \n",
    "#Sort all values in ascending order and collect them.\n",
    "\n",
    "calendar_not_leap_years_rdd = calendar_rdd.subtract(calendar_leap_years_rdd)\n",
    "calendar_not_leap_years_rdd.sortBy(lambda x:x,True).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e1476dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2nd Mill', <pyspark.resultiterable.ResultIterable at 0x7fe1632f7a60>),\n",
       " ('3rd Mill', <pyspark.resultiterable.ResultIterable at 0x7fe192fe6d90>)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.5. Group values of calendar_rdd by millennium (use the 2000 value for grouping).\n",
    "def mill_key(year):\n",
    "    if year >= 2000:\n",
    "        return \"3rd Mill\"\n",
    "    else:\n",
    "        return \"2nd Mill\"\n",
    "    \n",
    "grouped_rdd = calendar_rdd.groupBy(mill_key)\n",
    "grouped_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eb5a6087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1970,\n",
       " 1971,\n",
       " 1972,\n",
       " 1973,\n",
       " 1974,\n",
       " 1975,\n",
       " 1976,\n",
       " 1977,\n",
       " 1978,\n",
       " 1979,\n",
       " 1980,\n",
       " 1981,\n",
       " 1982,\n",
       " 1983,\n",
       " 1984,\n",
       " 1985,\n",
       " 1986,\n",
       " 1987,\n",
       " 1988,\n",
       " 1989,\n",
       " 1990,\n",
       " 1991,\n",
       " 1992,\n",
       " 1993,\n",
       " 1994,\n",
       " 1995,\n",
       " 1996,\n",
       " 1997,\n",
       " 1998,\n",
       " 1999,\n",
       " 2000,\n",
       " 2001,\n",
       " 2002,\n",
       " 2003,\n",
       " 2004,\n",
       " 2005,\n",
       " 2006,\n",
       " 2007,\n",
       " 2008,\n",
       " 2009,\n",
       " 2010,\n",
       " 2011,\n",
       " 2012,\n",
       " 2013,\n",
       " 2014,\n",
       " 2015,\n",
       " 2016,\n",
       " 2017,\n",
       " 2018,\n",
       " 2019,\n",
       " 2020,\n",
       " 2021,\n",
       " 2022,\n",
       " 2023,\n",
       " 2024,\n",
       " 2025,\n",
       " 2026,\n",
       " 2027,\n",
       " 2028,\n",
       " 2029,\n",
       " 2030,\n",
       " 2031,\n",
       " 2032,\n",
       " 2033,\n",
       " 2034,\n",
       " 2035,\n",
       " 2036,\n",
       " 2037,\n",
       " 2038,\n",
       " 2039,\n",
       " 2040,\n",
       " 2041,\n",
       " 2042,\n",
       " 2043,\n",
       " 2044,\n",
       " 2045,\n",
       " 2046,\n",
       " 2047,\n",
       " 2048,\n",
       " 2049,\n",
       " 2050,\n",
       " 2051,\n",
       " 2052,\n",
       " 2053,\n",
       " 2054,\n",
       " 2055,\n",
       " 2056,\n",
       " 2057,\n",
       " 2058,\n",
       " 2059,\n",
       " 2060,\n",
       " 2061,\n",
       " 2062,\n",
       " 2063,\n",
       " 2064,\n",
       " 2065,\n",
       " 2066,\n",
       " 2067,\n",
       " 2068,\n",
       " 2069,\n",
       " 2070,\n",
       " 2071,\n",
       " 2072,\n",
       " 2073,\n",
       " 2074,\n",
       " 2075,\n",
       " 2076,\n",
       " 2077,\n",
       " 2078,\n",
       " 2079,\n",
       " 2080,\n",
       " 2081,\n",
       " 2082,\n",
       " 2083,\n",
       " 2084,\n",
       " 2085,\n",
       " 2086,\n",
       " 2087,\n",
       " 2088,\n",
       " 2089,\n",
       " 2090,\n",
       " 2091,\n",
       " 2092,\n",
       " 2093,\n",
       " 2094,\n",
       " 2095,\n",
       " 2096,\n",
       " 2097,\n",
       " 2098,\n",
       " 2099]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.6. Imagine that mistakenly you deleted calendar_rdd using command “del calendar_rdd”. \n",
    "#Please try to get the same data using UNION command and save it to the new_calendar_rdd.\n",
    "new_calendar_rdd = calendar_leap_years_rdd.union(calendar_not_leap_years_rdd)\n",
    "\n",
    "new_calendar_rdd.sortBy(lambda x:x,True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba276c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.7. You want to make sure that the UNION command above didn’t produce any duplicated data. \n",
    "#Print count of elements of new_calendar_rdd and then a count of distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "40953f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_calendar_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f2dbf947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l= new_calendar_rdd.distinct().collect()\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "98866732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2970,\n",
       " 2971,\n",
       " 2972,\n",
       " 2973,\n",
       " 2974,\n",
       " 2975,\n",
       " 2976,\n",
       " 2977,\n",
       " 2978,\n",
       " 2979,\n",
       " 2980,\n",
       " 2981,\n",
       " 2982,\n",
       " 2983,\n",
       " 2984,\n",
       " 2985,\n",
       " 2986,\n",
       " 2987,\n",
       " 2988,\n",
       " 2989,\n",
       " 2990,\n",
       " 2991,\n",
       " 2992,\n",
       " 2993,\n",
       " 2994,\n",
       " 2995,\n",
       " 2996,\n",
       " 2997,\n",
       " 2998,\n",
       " 2999,\n",
       " 3000,\n",
       " 3001,\n",
       " 3002,\n",
       " 3003,\n",
       " 3004,\n",
       " 3005,\n",
       " 3006,\n",
       " 3007,\n",
       " 3008,\n",
       " 3009,\n",
       " 3010,\n",
       " 3011,\n",
       " 3012,\n",
       " 3013,\n",
       " 3014,\n",
       " 3015,\n",
       " 3016,\n",
       " 3017,\n",
       " 3018,\n",
       " 3019,\n",
       " 3020,\n",
       " 3021,\n",
       " 3022,\n",
       " 3023,\n",
       " 3024,\n",
       " 3025,\n",
       " 3026,\n",
       " 3027,\n",
       " 3028,\n",
       " 3029,\n",
       " 3030,\n",
       " 3031,\n",
       " 3032,\n",
       " 3033,\n",
       " 3034,\n",
       " 3035,\n",
       " 3036,\n",
       " 3037,\n",
       " 3038,\n",
       " 3039,\n",
       " 3040,\n",
       " 3041,\n",
       " 3042,\n",
       " 3043,\n",
       " 3044,\n",
       " 3045,\n",
       " 3046,\n",
       " 3047,\n",
       " 3048,\n",
       " 3049,\n",
       " 3050,\n",
       " 3051,\n",
       " 3052,\n",
       " 3053,\n",
       " 3054,\n",
       " 3055,\n",
       " 3056,\n",
       " 3057,\n",
       " 3058,\n",
       " 3059,\n",
       " 3060,\n",
       " 3061,\n",
       " 3062,\n",
       " 3063,\n",
       " 3064,\n",
       " 3065,\n",
       " 3066,\n",
       " 3067,\n",
       " 3068,\n",
       " 3069,\n",
       " 3070,\n",
       " 3071,\n",
       " 3072,\n",
       " 3073,\n",
       " 3074,\n",
       " 3075,\n",
       " 3076,\n",
       " 3077,\n",
       " 3078,\n",
       " 3079,\n",
       " 3080,\n",
       " 3081,\n",
       " 3082,\n",
       " 3083,\n",
       " 3084,\n",
       " 3085,\n",
       " 3086,\n",
       " 3087,\n",
       " 3088,\n",
       " 3089,\n",
       " 3090,\n",
       " 3091,\n",
       " 3092,\n",
       " 3093,\n",
       " 3094,\n",
       " 3095,\n",
       " 3096,\n",
       " 3097,\n",
       " 3098,\n",
       " 3099]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.8. Imagine that the time machine was created, and you are transported 1000 years ahead. \n",
    "#Apply a map function to each element of calendar_rdd and create a calendar_rdd_future.   \n",
    "calendar_rdd_future = calendar_rdd.map(lambda year: year + 1000)\n",
    "calendar_rdd_future.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
